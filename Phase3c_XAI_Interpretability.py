import os
import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from tqdm import tqdm

# --------------------------
# 1. æ ¸å¿ƒæ¶æ„å®šä¹‰ (ç¡®ä¿ä¸æ‚¨ Phase3a è®­ç»ƒä»£ç å®Œå…¨å¯¹é½)
# --------------------------
class FiLMLayer(nn.Module):
    def __init__(self, feature_dim, phys_dim):
        super().__init__()
        self.mlp = nn.Sequential(nn.Linear(phys_dim, phys_dim * 2), nn.SiLU(), nn.Linear(phys_dim * 2, feature_dim * 2))
    def forward(self, x, phys):
        params = self.mlp(phys).unsqueeze(-1).unsqueeze(-1)
        gamma, beta = torch.chunk(params, 2, dim=1)
        return x * gamma + beta

class MetamaterialFourierGemini(nn.Module):
    def __init__(self):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64), nn.SiLU()
        )
        self.phys_gate = FiLMLayer(64, 6)
        self.res_blocks = nn.Sequential(
            nn.Conv2d(64, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.SiLU(),
            nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128)
        )
        self.shortcut = nn.Conv2d(64, 128, 1, 2)
        self.attn = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Conv2d(128, 32, 1), nn.SiLU(), nn.Conv2d(32, 128, 1), nn.Sigmoid())
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.regressor = nn.Sequential(nn.Linear(128 + 6, 256), nn.SiLU(), nn.Dropout(0.1), nn.Linear(256, 1))

    def forward(self, pixel_values, physical_features):
        x = self.stem(pixel_values)
        x = self.phys_gate(x, physical_features)
        res = self.shortcut(x)
        x = F.silu(self.res_blocks(x) + res)
        x = x * self.attn(x)
        x_feat = self.global_pool(x).view(x.size(0), -1)
        logits = self.head_fusion(x_feat, physical_features)
        return logits

    def head_fusion(self, x_feat, physical_features):
        return self.regressor(torch.cat([x_feat, physical_features], dim=1))

# --------------------------
# 2. å…¨å±€ç»Ÿè®¡åˆ†æå¼•æ“
# --------------------------
def run_global_xai_analysis():
    # è·¯å¾„é…ç½®
    data_dir = "/inspire/hdd/global_user/zhongzhiyan-253108050052/Article_Panjy/Article_1/Data/1"
    res_file = os.path.join(data_dir, "ART_result.txt")
    # âœ… æŒ‡å‘æ‚¨è®­ç»ƒå¥½çš„æœ€ä¼˜ checkpoint
    model_weight_path = "/inspire/hdd/global_user/zhongzhiyan-253108050052/Article_Panjy/Article_1/output_phase3/checkpoint-best/pytorch_model.bin"
    output_dir = "./Phase3c_Global_XAI_Results"
    os.makedirs(output_dir, exist_ok=True)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # åŠ è½½æ•°æ®é›† (ä½¿ç”¨æ‚¨ Phase3a ä¸­çš„ Dataset ç±»é€»è¾‘)
    from Phase3a_Fourier_Gemini import FourierPhysicsDataset 
    dataset = FourierPhysicsDataset(data_dir, res_file)
    loader = DataLoader(dataset, batch_size=32, shuffle=False)

    # åˆå§‹åŒ–å¹¶åŠ è½½æœ€ä¼˜æ¨¡å‹
    model = MetamaterialFourierGemini().to(device)
    if os.path.exists(model_weight_path):
        # å…¼å®¹ Transformers Trainer çš„æƒé‡åŠ è½½
        state_dict = torch.load(model_weight_path, map_location=device)
        model.load_state_dict(state_dict, strict=False)
        print(f"âœ… å·²æˆåŠŸè°ƒç”¨æœ€ä¼˜æ¨¡å‹æƒé‡è¿›è¡Œæ·±åº¦åˆ†æã€‚")

    model.eval()
    global_saliency = np.zeros((224, 224))
    total_count = 0

    print("ğŸ“Š æ­£åœ¨æ‰§è¡Œå…¨é‡æ•°æ®åƒç´ å…³è”æ€§æ‰«æ...")
    for batch in tqdm(loader):
        pixel_values = batch["pixel_values"].to(device)
        phys_features = batch["physical_features"].to(device)
        
        # æ ¸å¿ƒï¼šå¼€å¯åƒç´ çº§æ¢¯åº¦è¿½è¸ª
        pixel_values.requires_grad = True
        
        logits = model(pixel_values, phys_features)
        
        # å¯¹ R å€¼æ±‚å’Œå¹¶åå‘ä¼ æ’­ï¼Œæå–â€œåƒç´ å½±å“åŠ›â€
        model.zero_grad()
        logits.sum().backward()
        
        # æå–ç©ºé—´åŸŸé€šé“ (Channel 0) çš„ç»å¯¹æ¢¯åº¦
        # è¿™ä»£è¡¨äº†æ¯ä¸ª (x,y) åƒç´ ç‚¹å¯¹æœ€ç»ˆ R å€¼çš„æ•æ„Ÿåº¦
        grads = pixel_values.grad.data.abs()[:, 0, :, :].cpu().numpy()
        global_saliency += np.sum(grads, axis=0)
        total_count += pixel_values.size(0)

    # 3. ç»Ÿè®¡å¹³å‡ä¸æ ‡å‡†åŒ–
    avg_saliency = global_saliency / total_count
    # å½’ä¸€åŒ–åˆ° [0, 1] æ–¹ä¾¿è§‚å¯Ÿç›¸å…³æ€§å¼ºåº¦
    norm_saliency = (avg_saliency - avg_saliency.min()) / (avg_saliency.max() - avg_saliency.min() + 1e-10)

    # 4. è¾“å‡ºåƒç´ ç‚¹å…³è”æƒé‡çŸ©é˜µ (.txt)
    # æ¯ä¸€è¡Œæ¯ä¸€åˆ—å¯¹åº”æ‚¨é«˜ç¨‹çŸ©é˜µçš„ä¸€ä¸ªåƒç´ ç‚¹
    txt_path = os.path.join(output_dir, "Global_Pixel_R_Correlation.txt")
    np.savetxt(txt_path, norm_saliency, delimiter='\t', fmt='%.6f')

    # 5. ç»¼åˆç›¸å…³æ€§çƒ­åŠ›å›¾å¯è§†åŒ–
    plt.figure(figsize=(10, 8))
    plt.imshow(norm_saliency, cmap='hot', interpolation='nearest')
    plt.colorbar(label='Pixel-to-R Correlation Weight')
    plt.title("Global Statistical Sensitivity Map\n(Where AI looks for Reflectivity)")
    plt.xlabel("Pixel X")
    plt.ylabel("Pixel Y")
    plt.savefig(os.path.join(output_dir, "Global_R_Correlation_Heatmap.png"), dpi=300)

    print(f"ğŸš€ åˆ†æå®Œæˆï¼")
    print(f"1. å…¨å±€åƒç´ å…³è”çŸ©é˜µå·²ä¿å­˜è‡³: {txt_path}")
    print(f"2. ç»¼åˆçƒ­åŠ›å›¾å·²ä¿å­˜è‡³: {output_dir}/Global_R_Correlation_Heatmap.png")

if __name__ == "__main__":
    run_global_xai_analysis()